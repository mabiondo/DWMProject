{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Dummy MODEL(S)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# For imports\n",
    "from notebooks import utility\n",
    "import importlib"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data import\n",
    "Let's import the data that was previously cleaned"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(\"../DWMProjectData/formodel/X_train.csv\")\n",
    "y_train = pd.read_csv(\"../DWMProjectData/formodel/y_train.csv\")\n",
    "X_valid = pd.read_csv(\"../DWMProjectData/formodel/X_valid.csv\")\n",
    "y_valid = pd.read_csv(\"../DWMProjectData/formodel/y_valid.csv\")\n",
    "X_test = pd.read_csv(\"../DWMProjectData/formodel/X_test.csv\")\n",
    "y_test = pd.read_csv(\"../DWMProjectData/formodel/y_test.csv\")\n",
    "# Transform all y in a 1-dimensional array - required to avoid warning in model building\n",
    "y_train = np.ravel(y_train)\n",
    "y_valid = np.ravel(y_valid)\n",
    "y_test = np.ravel(y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Score function\n",
    "\n",
    "I defined the score functions used for the regression. For a more clear approach I wrote the function `print_metrics` in the file `utility.py` In particular, I decided to write a function that prints the following values to compare models:\n",
    "    - mean absolute error\n",
    "- mean squared error\n",
    "- $r^2$, where the best score is 1, good is above 0.7\n",
    "- explained variance score, where the best score is 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "<module 'notebooks.utility' from 'C:\\\\Users\\\\marco\\\\Documents\\\\UNI\\\\Y3\\\\DataWebMining\\\\project\\\\DWMProject\\\\notebooks\\\\utility.py'>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utility import print_metrics\n",
    "importlib.reload(utility)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dummy Models\n",
    "Taking inspiration from [here](https://towardsdatascience.com/creating-benchmark-models-the-scikit-learn-way-af227f6ea977), I want to build first some dummy models whose results can be used to be compared with the real models.\n",
    "I build two models:\n",
    "- `dummy_mean` predicts as the mean of `y_train`\n",
    "- `dummy_median` predicts as the meadian of `y_train`"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< ---------- MEAN DUMMY ---------- >\n",
      "+--------------------------+--------+\n",
      "|          Method          | Value  |\n",
      "+==========================+========+\n",
      "| mean absolute error      | 0.071  |\n",
      "+--------------------------+--------+\n",
      "| mean squared error       | 0.030  |\n",
      "+--------------------------+--------+\n",
      "| r^2                      | -0.000 |\n",
      "+--------------------------+--------+\n",
      "| explained variance score | 0      |\n",
      "+--------------------------+--------+\n",
      "\n",
      "< --------- MEDIAN DUMMY --------- >\n",
      "+--------------------------+--------+\n",
      "|          Method          | Value  |\n",
      "+==========================+========+\n",
      "| mean absolute error      | 0.070  |\n",
      "+--------------------------+--------+\n",
      "| mean squared error       | 0.030  |\n",
      "+--------------------------+--------+\n",
      "| r^2                      | -0.003 |\n",
      "+--------------------------+--------+\n",
      "| explained variance score | 0      |\n",
      "+--------------------------+--------+\n"
     ]
    }
   ],
   "source": [
    "from utility import get_dummy_model\n",
    "importlib.reload(utility)\n",
    "\n",
    "dummy_mean = get_dummy_model(\"mean\", X_train, y_train)\n",
    "dummy_median = get_dummy_model(\"median\", X_train, y_train)\n",
    "\n",
    "y_pred = dummy_mean.predict(X_test)\n",
    "print(\"< ---------- MEAN DUMMY ---------- >\")\n",
    "print_metrics(y_test, y_pred)\n",
    "\n",
    "y_pred = dummy_median.predict(X_test)\n",
    "print(\"\\n< --------- MEDIAN DUMMY --------- >\")\n",
    "print_metrics(y_test, y_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As I could imagine, the results are pretty unsatisfactory, but that's totally fine, since the model is predicting as the mean or median.\n",
    "Now I just hope to obtain better results with my models!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}